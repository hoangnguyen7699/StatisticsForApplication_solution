\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}



\begin{document}
\title{Problem Set 10}
\author{Hoang Nguyen, Huy Nguyen}
\maketitle
    
\begin{problem}{1}
\item 1.
We have this result from problem 2 in Problem Set 3: 
\[ l(\theta) = \sum_{i=1}^{n} \Big ( -\frac{1}{2} log(\theta) -\frac{1}{2\theta} X_i^2\Big ) \]
Taking deravite ans set to equal 0, we get:
\[\frac{\partial l(\theta)}{\partial\theta} = \sum_{i=1}^{n}(-\frac{1}{2\theta} + \frac{1}{2\theta^2} X_i^2)=0 \Longleftrightarrow \theta^{MLE} = \frac{\sum_{i=1}^{n} X_i^2}{n} \tag{1}\]




\item 2. 
From (1), we have the second deraviate of log-likelihood:
\[\frac{\partial l(\theta)}{\partial \theta^2} =\sum_{i=1}^{n} \Big ( \frac{1}{2\theta^2} - \frac{1}{2\theta^3}X_i^2 \Big ) \Leftrightarrow I(\theta) = -\sum_{i=1}^{n}\mathbb{E}[\frac{1}{2\theta^2} - \frac{1}{2\theta^3}X_i^2]= \frac{n}{2\theta^2}\]
Hence,
\[(\theta^{MLE} - \theta)  \xrightarrow[n \rightarrow \infty]{\text{(d)}} N(0, \frac{2\theta^2}{n})  \]




\item 3.\\ 
a) From previous question, we know: $I(\theta) = \frac{n}{2\theta^2}$. Hence, $\pi(\theta) = c\sqrt{det I(\theta)} = c\sqrt{\frac{n}{2\theta^2}}$. This is improper prior\\
b) We have 
\[ \pi(\theta| X_1,..,X_n) = \frac{\pi(\theta) p_n(X_1,..,X_n|\theta))}{\int_{0}^{\infty} p_n(X_1,..,X_n|t)d\pi(t)} = \frac{c\sqrt{\frac{n}{2\theta^2}} \frac{1}{(2\pi\theta)^{\frac{n}{2}}} e^{-\frac{1}{2\theta} \sum_{i}^{n} X_i^2}}{\int_{0}^{\infty} p_n(X_1,..,X_n|t)d\pi(t)} = \frac{c\sqrt{n}}{(2\pi\theta)^{\frac{n}{2}} \sqrt{2\theta^2}} e^{-\frac{1}{2\theta} \sum_{i}^{n} X_i^2}\]
Hence, we can rewrite the posterio:
\[\frac{c\sqrt{n}}{(2\pi\theta)^{\frac{n}{2}} \sqrt{2\theta^2}} e^{-\frac{1}{2\theta} \sum_{i}^{n} X_i^2} = \frac{c\sqrt{n}}{(2\pi)^{\frac{n}{2}} \sqrt{2}} \frac{e^{-\frac{\frac{1}{2} \sum_{i=1}^{n}}{\theta}}}{\theta^{\frac{n}{2} +1}} = \frac{c\sqrt{n}}{(2\pi)^{\frac{n}{2}} \sqrt{2}} \frac{e^{-\frac{\beta}{\theta}}}{\theta^{\alpha+1}} \]
Where $\alpha = \frac{n}{2}$, $\beta = \frac{\sum_{i=1}^{n} X_i^2}{2}$ and $c$ is the constant satisfied $\frac{c\sqrt{n}}{(2\pi)^{\frac{n}{2}} \sqrt{2}} = \frac{\beta^{\alpha}}{\tau(\alpha)}$. This posterior is Gamma distribution with parameters $\alpha$ and $\beta$.
c)\\
We know $\hat{\theta^{(\pi)}}= \int_{0}^{\infty} \theta d \pi(\theta| X_1,..,X_n)$ is the expectation of Gamma distribution with parameters $\alpha$ and $\beta$. Hence, $\hat{\theta^{(\pi)}} = \frac{\beta}{\alpha - 1} = \frac{\sum_{i=1}^{n} X_i^2}{n -2}$.


\end{problem}

\begin{problem}{2}
\item 1. Conditionally on $\beta$, we have $Y-X\beta \sim N_p(0, \sigma^2 I_n)$. Hence, $Y \sim N_p(X\beta, \sigma^2 I_n)$.
\item 2.\\
a) We know:
\[ \pi(\beta| X_1,..,X_n) \propto  \pi(\beta) p_n(X_1,..,X_n|\beta)) \propto e^{\frac{1}{\sigma^2} ||Y - X\beta||_2^{2}} e^{\frac{1}{\tau^2}||\beta||_2^2} = e^{\frac{1}{\sigma^2} \Big ( ||Y - X\beta||_2^{2} + \frac{\sigma^2}{\tau^2} ||\beta||_2^2 \Big ) } \]
b) Let $\pi(\beta| X_1,..,X_n) = Ke^{\frac{1}{\sigma^2} \Big ( ||Y - X\beta||_2^{2} + \frac{\sigma^2}{\tau^2} ||\beta||_2^2 \Big ) } $\\
We know: 
\[\frac{1}{\sigma^2} \Big ( ||Y - X\beta||_2^{2} + \frac{\sigma^2}{\tau^2} ||\beta||_2^2 \Big ) = \frac{1}{\sigma^2}\Big ( (Y - X\beta)^{T}(Y - X\beta)  + \frac{\sigma^2}{\tau^2} \beta^{T}\beta \Big )\]
\[ = \frac{1}{\sigma^2}\Big ( Y^{T}Y - 2\beta^{T}X^{T}Y + \beta^{T}X^{T}X\beta  + \frac{\sigma^2}{\tau^2} \beta^{T}\beta \Big ) =  \frac{1}{\sigma^2}\Big ( Y^{T}Y - 2\beta^{T}X^{T}Y \Big )+ \beta^{T}(\frac{1}{\sigma^2} X^{T}X + \frac{1}{\tau^2} I)\beta \]
Hence, 
\[ \pi(\beta| X_1,..,X_n) = Ke^{\frac{1}{\sigma^2}\Big ( Y^{T}Y - 2\beta^{T}X^{T}Y\Big ) + \beta^{T}(\frac{1}{\sigma^2} X^{T}X + \frac{1}{\tau^2} I)\beta} = K_1e^{\frac{-2}{\sigma^2}\beta^{T}X^{T}Y + \beta^{T}(\frac{1}{\sigma^2} X^{T}X + \frac{1}{\tau^2} I)\beta }\]
\[= K_1e^{ \beta^{T}(\frac{1}{\sigma^2} X^{T}X + \frac{1}{\tau^2} I)\beta -\frac{2}{\sigma^2}\beta^{T}X^{T}Y   }\]
Let $\sum^{-1} = \frac{1}{\sigma^2} X^{T}X + \frac{1}{\tau^2} I$ and $\mu = (X^{T}X + \frac{\sigma^2}{\tau^2} I)^{-1}X^{T}y$. We can see that $\frac{1}{\sigma^2} X^{T}y = \sum^{-1}\mu$. Thereforce, we can rewrite
\[ \pi(\beta| X_1,..,X_n) =  K_1 e^{\beta^T \sum^{-1} \beta - 2\beta \sum^{-1}\mu } = K_1 e^{\beta^T \sum^{-1} \beta - 2\beta \sum^{-1}\mu }= K_1 e^{\beta^T \sum^{-1} \beta - 2\beta \sum^{-1}\mu + \mu^T\sum^{-1}\mu - \mu^T\sum^{-1}\mu}\]
We see that $ \mu^T\sum^{-1}\mu$ depends on $X$ and $Y$ which are constants, hence we can get rid this constant by rewrite:
\[\pi(\beta| X_1,..,X_n) = K_2 e^{\beta^T \sum^{-1} \beta - 2\beta \sum^{-1}\mu + \mu^T\sum^{-1}\mu} = K_2e^{(\beta - \mu)^{T}\sum^{-1}(\beta - \mu)}\]
c)) The posterior mean of $\beta$ is the expectation of $g(\beta)$ which is $\mu = (X^{T}X + \frac{\sigma^2}{\tau^2} I)^{-1}X^{T}y$



\end{problem}
\begin{problem}{3}


\end{problem}

\end{document}