\documentclass[10pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}



\begin{document}
\title{Problem Set 2}
\author{Hoang Nguyen, Huy Nguyen}
\maketitle
    
\begin{problem}{1}
\item 1.
We know $\mathbb{E}[X_i]=1.p+ 0.(1-p)=p$ and $\mathbb{E}[X_i^2]=1.p+ 0.(1-p)=p$. Hence, $\mathbb{V}[X_i]= \mathbb{E}[X_i^2]- (\mathbb{E}[X_i])^2 =p(1-p).$
\item 2. \textit{Theorem}: If bias $\rightarrow$ 0 ans se (standard error) $\rightarrow$ 0 as n $\rightarrow \infty$ then $\hat{\theta}$ is consistent, that is, $\hat{\theta} \xrightarrow{\textbf{P}} \theta$.\\




$\mathbb{V}[\bar{X_i}(1-\bar{X_i})]=\mathbb{V}[\frac{\sum_{i=1}^{n} X_i}{n}(1-\frac{\sum_{i=1}^{n} X_i}{n})]=\mathbb{V}[\frac{1}{n^2} \sum_{i=1}^{n} X_i \Big(n-\sum_{i=1}^{n} X_i\Big) ]$ 
\[=\frac{1}{n^4} \mathbb{V}[\sum_{i=1}^{n} X_i \Big(n-\sum_{i=1}^{n} X_i\Big)]\]
\[=\frac{1}{n^4} \mathbb{V}\Big[ \sum_{i=1}^{n} X_i \sum_{i=1}^{n} (1-X_i) \Big]\]

\[=\frac{1}{n^4} \mathbb{V}\Big[\sum_{i=1}^{n} X_i(1-X_i) +\sum_{i \neq j} X_i(1-X_j) \Big] \]

$X_i \in {0,1 } \Rightarrow \sum_{i=1}^{n} X_i(1-X_i)=0$ 
\\
Hence, 

\[\mathbb{V}[\bar{X_i}(1-\bar{X_i})]= \frac{1}{n^4} \mathbb{V} \Big[ \sum_{i \neq j} X_i(1-X_j) \Big] \]

By $X_i$ is IID:
 \[\mathbb{V}[\bar{X_i}(1-\hat{\bar{X_i}})]= \frac{1}{n^4} \sum_{i \neq j} \mathbb{V} [X_i]\mathbb{V}[1-X_i] \]
\[= \frac{1}{n^4} n(n-1)p(p-1) \]
The last equality tends to 0 as n $\rightarrow \infty$. So we have  $\mathbb{V}[\bar{X_i}(1-\bar{X_i})] \rightarrow$ 0. \\
Hence, se($\bar{X_i}(1-\bar{X_i})$)=$\sqrt{\mathbb{V}[\bar{X_i}(1-\bar{X_i})]} \rightarrow$ 0. (1)\\
By similar method, we can esaily get: $\mathbb{E}[\bar{X_i}(1-\bar{X_i})]= \frac{1}{n^2} \sum_{i \neq j} \mathbb{E}[X_i(1-X_j)] = \frac{n(n-1)}{n^2} p(1-p) \rightarrow p(1-p)$ as n $\rightarrow \infty$.\\
Hence, bias$\Big(\bar{X_i}(1-\bar{X_i})\Big)= \mathbb{E}[\bar{X_i}(1-\bar{X_i})] - p(1-p) \rightarrow 0$ as n $\rightarrow \infty$  (2)\\
From (1), (2) and the theorem above, we get $\bar{X_i}(1-\bar{X_i})$ is a consistent estimator of p(1-p).

\item 3. We actually complete this exercise in previous solution.\\
bias$\Big(\bar{X_i}(1-\bar{X_i})\Big)= \mathbb{E}[\bar{X_i}(1-\bar{X_i})] - p(1-p)= \frac{n(n-1)}{n^2} p(1-p) -p(1-p)$.
\item 4. To find an unbiased estimator, we have to find x such that $\frac{xn(n-1)}{n^2}=1 \Rightarrow x=\frac{n}{n-1}$.\\
Hence, an unbiased estimator can be $\frac{n}{n-1}\bar{X_i}(1-\bar{X_i})$





\end{problem}

\begin{problem}{2}
\item 1.
$(\mathbb{N}, (Poiss(\lambda))_{\lambda>0})$. This paramter is identified.
\item 2.
$(\mathbb{R_+}, (Exp(\lambda))_{10>\lambda>0})$. This parameter is identified.
\item 3.
$(\mathbb{R_+}, (Uni(0, \theta))_{\theta >0})$. This parameter is identified.
\item 4.
$(\mathbb{R}, (N(\mu, \sigma^2))_{(\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R_+}})$. These parater are identified.
\item 5.
\[\mathbb{P}(N(\mu, \sigma^2)>0)=\mathbb{P}\Big( N(0,1) > \frac{-\mu}{\sigma^2} \Big)=\phi(\frac{\mu}{\sigma^2})\]
Hence, the statistical model is: $(\{0,1\}, (Ber(\phi(\frac{\mu}{\sigma^2}))_{(\mu, \sigma^2) \in \mathbb{R} \times \mathbb{R_+}})$. This model depends on $\frac{\mu}{\sigma^2} \Rightarrow$ these parameters are not identified.
\item 6.
Same for 3.
\item 7.
Let X $\thicksim Exp(\lambda) \Rightarrow \mathbb{P}(X>20)=e^{-20\lambda}$. Hence, the statistical model is:
\[(\{ 0,1\},(Ber(e^{-20\lambda}))_{\lambda>0}) \] 
This parameter is identified.

\item 8.
Let X $\thicksim Ber(p)$ such that:
\begin{align}
    \begin{cases}
        X_{i}=1 \text{ if machine i has timelife less than 500 days} \\
        X_{i}=0 \text{ otherwise}
    \end{cases}
\end{align}

Hence: 
\[p=\mathbb{P}(X_{i}=1)=1-e^{-500\lambda}\]
The number of machines that have stopped working before 500 days is a binominal random variable with parameter (67, $1-e^{-500\lambda}$)\\
The statistical model is $(\{1,2,3,..,67 \}, (Binominal(67, 1-e^{-500\lambda}))_{\lambda>0})$. This parameter is identified.

\end{problem}

\begin{problem}{3}
\item 1.
By central limit theorem (CLT), we have: 
\[ \frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma} \thicksim \text( N(0,1))\]
Hence, $(a_{n})_{n \in \mathbb{N}}$ can be $\frac{\sqrt{n}}{\sigma}$ and $(b_{n})_{n \in \mathbb{N}}$ can be $\mu$.

\item 2.
We have: $Z \backsim N(0,1)$\\
Hence, $\mathbb{P}[|Z| \leqslant t]=\mathbb{P}[-t\leqslant Z \leqslant t]= \phi(t)-\phi(-t)= \phi(t)-(1-\phi(t))=2\phi(t)-1= 2\mathbb{P}[Z \leqslant t]-1$.
\item 3.
From part 1 we get: 
\[ \frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma} \thicksim \text( N(0,1))\]
from part 2 we get:
\[ \mathbb{P}[|Z| \leqslant t]=2\mathbb{P}[Z \leqslant t]-1\]
Substitution:
\[ \mathbb{P}\Big[|\frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma}| \leqslant t\Big]=2\mathbb{P}[Z \leqslant t]-1\]
We have $2\mathbb{P}[Z \leqslant t]-1=0.95 \Rightarrow t=\phi^{-1}(\frac{0.95+1}{2})=1.96$. \\
Hence, 
\[ \mathbb{P}\Big[|\frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma}| \leqslant 1.96\Big]= 0.95\]

\[  \mathbb{P}\Big[-\frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma} \leqslant 1.96 \leqslant \frac{\sqrt{n}(\bar{X_i}-\mu)}{\sigma}\Big]= 0.95\]
Because $X_i$ is Poisson random variable with parameter $\lambda$, so $ \mu=\lambda$ and $\sigma= \sqrt{\lambda}$\\
We get:
\[  \mathbb{P}\Big[-\frac{\sqrt{n}(\bar{X_i}-\lambda)}{\sqrt{\lambda}} \leqslant 1.96 \leqslant \frac{\sqrt{n}(\bar{X_i}-\lambda)}{\sqrt{\lambda}}\Big]= 0.95\]

\[ \mathbb{P}\Big[\bar{X_i}- \frac{1.96\sqrt{\lambda}}{\sqrt{n}} \leqslant \lambda \leqslant \bar{X_i}+ \frac{1.96\sqrt{\lambda}}{\sqrt{n}}  \Big] =0.95\]

We know: $\bar{X_i} \xrightarrow{P} \mathbb{E}[\bar{X_i}]=\lambda$\\
Hence, 
\[ \mathbb{P}\Big[\bar{X_i}- \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}} \leqslant \lambda \leqslant \bar{X_i}+ \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}}  \Big] \geqslant 0.95\]
$\Rightarrow$ L=[$\bar{X_i}- \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}}, \bar{X_i}+ \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}}$] 
\item 4.
We can easily see min($X_i$)$\leqslant \bar{X_i} \leqslant$ max($X_i$). Hence, a new interval can be: 
\[L=[min(X_i)- \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}}, max(X_i)+ \frac{1.96\sqrt{\bar{X_i}}}{\sqrt{n}}]\]
\end{problem}
\begin{problem}{4}
We have $X_i$ is IID. Hence, $\mathbb{P}(M_n \leqslant t)=\prod_{n=1}^{n}\mathbb{P}(X_i\leqslant t).$\\
By uniform distribution, the CDF of $M_n$:
\[ \mathbb{P}(M_n \leqslant t)=F(t)=\Big(\frac{t}{\theta}\Big)^n \]
Hence, the PDF of $M_n$ is:
\[f(t)=\frac{dF}{dt}=n\theta^{-n}t^{n-1} \]
We can easily get:
\[\mathbb{E}[M_n]= \int_{0}^{\theta} tn\theta^{-n}t^{n-1}dt=\frac{n}{n+1}\theta \rightarrow \theta \textbf{ as n}\rightarrow \infty \]
By Markov's Inequality:
\[ \mathbb{P}\Big[ | M_n -\theta|> \epsilon \Big]\leqslant \mathbb{P}[M_n-\theta>\epsilon] \leqslant \frac{\mathbb{E}[M_n-\theta]}{\epsilon}=\frac{\mathbb{E}[M_n]-\theta}{\epsilon} \rightarrow 0\]
Hence, $M_n$ converages in probility to $\theta$.

\item 2.
From part 1 we get: $M_n$: $\mathbb{P}[M_n \leqslant t]=\Big(\frac{t}{\theta}\Big)^n$. Hence, CDF of $n(1-\frac{M_n}{\theta})$ is:
\[ P\Big[n(1-\frac{M_n}{\theta})\leqslant t\Big]=\mathbb{P}\Big[M_n \geqslant \frac{(n-t)\theta}{n} \Big] =1-\Big(\frac{n-t}{n}\Big)^{n} \rightarrow 1- e^{-t} \textbf{ as n}  \rightarrow \infty\]

Hence, $n(1-\frac{M_n}{\theta})$ converages in distribution to an exponential random variable with parameter 1.
\item 3.
Let A is an exponential random variable with parameter 1. Because $n(1-\frac{M_n}{\theta})$ converages in distribution to X, we have:
\[\mathbb{P}\Big[ n(1-\frac{M_n}{\theta}) \leqslant t\Big] \rightarrow \mathbb{P}[X\leqslant t]= 1-e^{-t} \]
$1-e^{-t}=0.95 \Rightarrow t=3$. We have:
\[ \mathbb{P}\Big[ n(1-\frac{M_n}{\theta}) \leqslant 3\Big] \rightarrow 0.95\]
which is:
\[ \mathbb{P}\Big[ \theta \leqslant \frac{nM_n}{n-3}\Big] \rightarrow 0.95\]
On the other hand, we always have $\theta \geqslant M_n$ (uniform distribution). Hence, we get: 
\[ \mathbb{P}\Big[M_n \leqslant \theta \leqslant \frac{nM_n}{n-3}\Big] \rightarrow 0.95 \textbf{ as n} \rightarrow \infty\]
We conclude $L=\Big[M_n, \frac{nM_n}{n-3} \Big]=\Big[ M_n, M_n+ \frac{3M_n}{n-3} \Big]$.
\item 4.
$bias(M_n)=\mathbb{M_n}-\theta= \frac{n}{n+1}\theta -\theta \neq 0$. Hence, $M_n$ is biased.
\end{problem}
\end{document}